{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkV4thKyn3EP",
        "outputId": "bfc850bc-735d-4eb0-c8a6-e6e7ae2a744d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "40paQfW63zIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zfEdts0pYBcg",
        "outputId": "843c5f19-4607-46e7-c659-0bc8b2935c6b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/ESG_wordlist.xlsx - ESG-Wordlist.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2f673ef2699c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load CSV file (replace with actual file name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/ESG_wordlist.xlsx - ESG-Wordlist.csv\"\u001b[0m  \u001b[0;31m# Change this to your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize Porter Stemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ESG_wordlist.xlsx - ESG-Wordlist.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load CSV file (file name)\n",
        "file_name = \"/content/ESG_wordlist.xlsx - ESG-Wordlist.csv\"\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Assuming words are in the first column, apply stemming\n",
        "df[\"Stemmed_Words\"] = df.iloc[:, 0].astype(str).apply(stemmer.stem)\n",
        "\n",
        "# Save output to a new CSV\n",
        "output_file = \"stemmed_words.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Processed words saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.request import urlretrieve\n",
        "from datetime import datetime as dt\n",
        "import os, re\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "import codecs\n",
        "# f=codecs.open(\"./data/AAPL/20151028.htm\", 'r')\n",
        "\n",
        "data_format = '10-k'\n",
        "DOMIN = 'https://www.sec.gov'\n",
        "\n",
        "\n",
        "\n",
        "class TenKDownloader:\n",
        "    def __init__(self, cik, start_date, end_date):\n",
        "        if type(cik) == str:\n",
        "            cik = [cik]\n",
        "        elif type(cik) == list:\n",
        "            for i, ele in enumerate(cik):\n",
        "                assert type(ele)==str, f'cik at index {i} is not string: %s'%type(ele)\n",
        "        else:\n",
        "            raise TypeError('CIK should be string or list of string, input type is %s'%type(cik))\n",
        "        self.CIK = cik\n",
        "        self.start_date = dt.strptime(start_date,'%Y%m%d')\n",
        "        self.end_date = dt.strptime(end_date,'%Y%m%d')\n",
        "        self.all_url = {}\n",
        "        self.cwd = os.getcwd()\n",
        "\n",
        "    def download(self, target = './data', reset_flag=False):\n",
        "        os.chdir(self.cwd)\n",
        "        os.chdir(target)\n",
        "        for c in self.CIK:\n",
        "            try:\n",
        "                if reset_flag:\n",
        "                    result = self._search_each(c)\n",
        "                else:\n",
        "                    if c in self.all_url:\n",
        "                        continue\n",
        "                    else:\n",
        "                        result = self._search_each(c)\n",
        "            except ValueError as info:\n",
        "                print(info)\n",
        "                continue\n",
        "            try:\n",
        "                os.mkdir(c)\n",
        "            except FileExistsError:\n",
        "                pass\n",
        "            os.chdir(f'./{c}')\n",
        "            for each in result:\n",
        "                print(f'Downloading {c}:{each[0]} {each[1]}')\n",
        "                filename = each[0]+str(each[each[1].rfind('.'):])\n",
        "                urlretrieve(each[1], filename)\n",
        "                print('File saved in {}'.format(os.getcwd()+'\\\\'+filename))\n",
        "            self.all_url[c] = result\n",
        "            os.chdir('..')\n",
        "        os.chdir('..')\n",
        "\n",
        "    def _search_each(self, cik):\n",
        "        assert cik in self.CIK, '%s is not in CIK list'%cik\n",
        "        url = f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=10-k&dateb=&owner=exclude&count=40'\n",
        "        search_page = requests.get(url)\n",
        "        assert search_page.status_code == 200, 'request code for search page: %s' % search_page.status_code\n",
        "        search_head = BeautifulSoup(search_page.content, 'html.parser')\n",
        "        search_result = search_head.select('table.tableFile2 tr')\n",
        "        if len(search_result)==0:\n",
        "            raise ValueError(f'Result for {cik} is not available, {url}')\n",
        "        search_result.pop(0)\n",
        "        start_idx, end_idx = self._search_date([self._get(item, 'date') for item in search_result], self.start_date, self.end_date)\n",
        "        # print(start_idx, end_idx)\n",
        "        result = []\n",
        "        for i in range(start_idx, end_idx+1):\n",
        "            if self._get(search_result[i], 'type')!='10-K':\n",
        "                # print(self._get(search_result[i], 'date').strftime('%Y%m%d'))\n",
        "                continue\n",
        "            date = self._get(search_result[i], 'date').strftime('%Y%m%d')\n",
        "            sub_url = DOMIN + search_result[i].find('a', attrs={\"id\": \"documentsbutton\"})['href']\n",
        "            company_page = requests.get(sub_url)\n",
        "            assert  company_page.status_code == 200, 'request code for company page: %s' % company_page.status_code\n",
        "            company_head = BeautifulSoup(company_page.content, 'html.parser')\n",
        "            file_table = company_head.select('table.tableFile')[0].select('tr')\n",
        "            file_table.pop(0)\n",
        "            for item in file_table:\n",
        "                if '10-K' in item.select('td')[3].contents[0]:\n",
        "                    break\n",
        "            doc_url = item.select('td a')[0]['href']\n",
        "            result.append((date, DOMIN+doc_url))\n",
        "        return result\n",
        "\n",
        "    def _get(self, item, info):\n",
        "        if info == 'date':\n",
        "            date = item.select('td')[3].contents[0]\n",
        "            ret = dt.strptime(date,'%Y-%m-%d')\n",
        "        elif info == 'url':\n",
        "            ret = DOMIN + item.find('a', attrs={\"id\": \"documentsbutton\"})['href']\n",
        "        elif info == 'type':\n",
        "            ret = item.select('td')[0].contents[0]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return ret\n",
        "\n",
        "    def _search_date(self, ls, start, end):\n",
        "        h, t = ls[-1], ls[0]\n",
        "        n = len(ls)\n",
        "        assert start <= t and end >= h, f'Available time interval: {h} to {t}, input: {start} to {end}'\n",
        "        # print(h,t)\n",
        "        if start >= h:\n",
        "            ei, _ = self._bsearch_dec(ls, start)\n",
        "        else:\n",
        "            ei = len(ls)-1\n",
        "        if end <= t:\n",
        "            _, si = self._bsearch_dec(ls, end)\n",
        "        else:\n",
        "            si = 0\n",
        "        return si, ei\n",
        "\n",
        "    def _bsearch_dec(self, ls, point):\n",
        "        a = 0\n",
        "        b = len(ls)\n",
        "        while b-a > 1:\n",
        "            tmp = int((a+b)/2)\n",
        "            if ls[tmp] >= point:\n",
        "                a = tmp\n",
        "            else:\n",
        "                b = tmp\n",
        "        return a,b\n",
        "\n",
        "\n",
        "class TenKScraper:\n",
        "    def __init__(self, section, next_section):\n",
        "        self.all_section = [str(i) for i in range(1, 16)] + ['1A', '1B', '7A', '9A', '9B']\n",
        "        section = re.findall(r'\\d.*\\w*', section.upper())[0]\n",
        "        next_section = re.findall(r'\\d.*\\w*', next_section.upper())[0]\n",
        "        if section  not in self.all_section:\n",
        "            raise ValueError(f'Section: {section} is not avaiable, avaiable section: {self.all_section}')\n",
        "        if next_section  not in self.all_section:\n",
        "            raise ValueError(f'Section: {next_section} is not avaiable, avaiable section: {self.all_section}')\n",
        "        self.section = 'Item ' + section\n",
        "        self.next_section = 'Item ' + next_section\n",
        "\n",
        "    def scrape(self, input_path, output_path):\n",
        "        with open(input_path, 'rb') as input_file:\n",
        "            page = input_file.read()  # <===Read the HTML file into Python\n",
        "            # Pre-processing the html content by removing extra white space and combining then into one line.\n",
        "            page = page.strip()  # <=== remove white space at the beginning and end\n",
        "            page = page.replace(b'\\n', b' ')  # <===replace the \\n (new line) character with space\n",
        "            page = page.replace(b'\\r', b'')  # <===replace the \\r (carriage returns -if you're on windows) with space\n",
        "            page = page.replace(b'&nbsp;',\n",
        "                                b' ')  # <===replace \"&nbsp;\" (a special character for space in HTML) with space.\n",
        "            page = page.replace(b'&#160;',\n",
        "                                b' ')  # <===replace \"&#160;\" (a special character for space in HTML) with space.\n",
        "\n",
        "            while b'  ' in page:\n",
        "                page = page.replace(b'  ', b' ')  # <===remove extra space\n",
        "\n",
        "            # Using regular expression to extract texts that match a pattern\n",
        "\n",
        "            # Define pattern for regular expression.\n",
        "            # The following patterns find ITEM 1 and ITEM 1A as diplayed as subtitles\n",
        "            # (.+?) represents everything between the two subtitles\n",
        "            # If you want to extract something else, here is what you should change\n",
        "\n",
        "            # Define a list of potential patterns to find ITEM 1 and ITEM 1A as subtitles\n",
        "            p1 = bytes(r'bold;\\\">\\s*' + self.section + r'\\.(.+?)bold;\\\">\\s*' + self.next_section + r'\\.',\n",
        "                       encoding='utf-8')\n",
        "            p2 = bytes(r'b>\\s*' + self.section + r'\\.(.+?)b>\\s*' + self.next_section + r'\\.', encoding='utf-8')\n",
        "            p3 = bytes(r'' + self.section + r'\\.\\s*<\\/b>(.+?)' + self.next_section + r'\\.\\s*<\\/b>', encoding='utf-8')\n",
        "            p4 = bytes(r'' + self.section + r'\\.\\s*[^<>]+\\.\\s*<\\/b(.+?)' + self.next_section + r'\\.\\s*[^<>]+\\.\\s*<\\/b',\n",
        "                       encoding='utf-8')\n",
        "            p5 = bytes(r'b>\\s*<font[^>]+>\\s*' + self.section + r'\\.(.+?)b>\\s*<font[^>]+>\\s*' + self.next_section + r'\\.', encoding='utf-8')\n",
        "            p6 = bytes(r'' + self.section.upper() + r'\\.\\s*<\\/b>(.+?)' + self.next_section.upper() + r'\\.\\s*<\\/b>', encoding='utf-8')\n",
        "\n",
        "            p7 = bytes(r'underline;\\\">\\s*' + self.section + r'\\<\\/font>(.+?)underline;\\\">\\s*'+ self.next_section + r'\\.\\s*\\<\\/font>',encoding = 'utf-8')\n",
        "            p8 = bytes(r'underline;\\\">\\s*' + self.section + r'\\.\\<\\/font>(.+?)underline;\\\">\\s*'+ self.next_section + r'\\.\\s*\\<\\/font>',encoding = 'utf-8')\n",
        "            p9 = bytes(r'<font[^>]+>\\s*' + self.section + r'\\:(.+?)\\<font[^>]+>\\s*'+self.next_section + r'\\:\\s*',encoding = 'utf-8')\n",
        "            p10 = bytes(r'<font[^>]+>\\s*' + self.section + r'\\.\\<\\/font>(.+?)\\<font[^>]+>\\s*' + self.next_section + r'\\.',encoding = 'utf-8')\n",
        "            p11 = bytes(r'' + self.section + r'\\.(.+?)<font[^>]+>\\s*' + self.next_section + r'\\.\\<\\/font>',encoding = 'utf-8')\n",
        "            p12 = bytes(r'b>\\s*<font[^>]+>\\s*' + self.section + r'(.+?)b>\\s*<font[^>]+>\\s*' + self.next_section + r'\\s*\\<\\/font>', encoding='utf-8')\n",
        "            p13 = bytes(r'' + self.section + r'\\.\\s*[^<>]+\\.\\s*<\\/b(.+?)b>\\s*' + self.next_section + r'\\.',\n",
        "                       encoding='utf-8')\n",
        "            regexs = (\n",
        "                p1,  # <===pattern 1: with an attribute bold before the item subtitle\n",
        "                p2,  # <===pattern 2: with a tag <b> before the item subtitle\n",
        "                p3,  # <===pattern 3: with a tag <\\b> after the item subtitle\n",
        "                p4,  # <===pattern 4: with a tag <\\b> after the item+description subtitle\n",
        "                p5,  # <===pattern 5: with a tag <b><font ...> before the item subtitle\n",
        "                p6,  # <===pattern 6: with a tag <\\b> after the item subtitle (ITEM XX.<\\b>)\n",
        "                p7,\n",
        "                p8,\n",
        "                p9,\n",
        "                p10,\n",
        "                p11,\n",
        "                p12,\n",
        "                p13)\n",
        "\n",
        "            # Now we try to see if a match can be found...\n",
        "            for regex in regexs:\n",
        "                match = re.search(regex, page,\n",
        "                                  flags=re.IGNORECASE)  # <===search for the pattern in HTML using re.search from the re package. Ignore cases.\n",
        "\n",
        "                # If a match exist....\n",
        "                if match:\n",
        "                    # Now we have the extracted content still in an HTML format\n",
        "                    # We now turn it into a beautiful soup object\n",
        "                    # so that we can remove the html tags and only keep the texts\n",
        "\n",
        "                    soup = BeautifulSoup(match.group(1),\n",
        "                                         \"html.parser\")  # <=== match.group(1) returns the texts inside the parentheses (.*?)\n",
        "\n",
        "                    # soup.text removes the html tags and only keep the texts\n",
        "                    rawText = soup.text.encode('utf8')  # <=== you have to change the encoding the unicodes\n",
        "\n",
        "                    # remove space at the beginning and end and the subtitle \"business\" at the beginning\n",
        "                    # ^ matches the beginning of the text\n",
        "                    # outText = re.sub(b\"^business\\s*\", b\"\", rawText.strip(), flags=re.IGNORECASE)\n",
        "                    Path(output_path).touch()\n",
        "                    with open(output_path, \"wb\") as output_file:\n",
        "                        output_file.write(rawText)\n",
        "\n",
        "                    break  # <=== if a match is found, we break the for loop. Otherwise the for loop continues\n",
        "\n",
        "        if match is None:\n",
        "            print(f'No matched sections: {self.section}, {self.next_section} found in {input_path}.')\n",
        "            return None\n",
        "        else:\n",
        "            return rawText\n",
        "\n",
        "if __name__=='__main__':\n",
        "    pass\n",
        "    # print(os.getcwd())\n",
        "    # os.chdir('./BIA660_project')\n",
        "    # company_name = ['AAPL', 'GOOG']\n",
        "    # downloader = TenKDownloader(company_name, '20150101','20180101')\n",
        "    # downloader.download()\n",
        "\n",
        "    # scraper = TenKScraper('Item 1A', 'Item 1B')  # scrape text start from Item 1A, and stop by Item 1B\n",
        "    # scraper2 = TenKScraper('Item 7', 'Item 8')\n",
        "    # scraper.scrape('./data/1326160/20110225.htm', './data/txt/test.txt')\n",
        "    # scraper2.scrape('./data/1326160/20110225.htm', './data/txt/test2.txt')"
      ],
      "metadata": {
        "id": "WNhx5l2Whgwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to your database\n",
        "conn = sqlite3.connect(\"database.sqlite\")  # Replace with your actual database file\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "# tables = cursor.fetchall()\n",
        "# print(tables)\n",
        "# Fetch company names and CIK numbers\n",
        "cursor.execute(\"SELECT name, cik_key FROM companies\")  # Replace 'your_table' with actual table name\n",
        "data = cursor.fetchall()\n",
        "\n",
        "# Convert to a DataFrame for easier handling\n",
        "df = pd.DataFrame(data, columns=[\"name\", \"cik_key\"])\n",
        "print(df.head())  # Display first few rows\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1CyxtrSUl2Ap",
        "outputId": "bfe16bb9-1235-4ac9-ebc9-2ca5675ef392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "no such table: companies",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-13b83f480dae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(tables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Fetch company names and CIK numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT name, cik_key FROM companies\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'your_table' with actual table name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: companies"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract CIKs as a list of strings\n",
        "cik_list = df[\"cik_key\"].astype(str).tolist()\n",
        "import os\n",
        "\n",
        "# Define target directory\n",
        "target_dir = \"/content/data\"  # Use absolute path in Colab\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Now run the downloader with the correct path\n",
        "downloader.download(target=target_dir, reset_flag=False)\n",
        "\n",
        "# Set your desired date range\n",
        "start_date = \"20130101\"  # Adjust to your required range\n",
        "end_date = \"20191231\"\n",
        "\n",
        "# Initialize and run the downloader\n",
        "downloader = TenKDownloader(cik_list, start_date, end_date)\n",
        "downloader.download(target=\"./data\", reset_flag=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8Iwa-qROoej8",
        "outputId": "4aa18487-a06f-409b-e04c-c35f81b9ebf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-724e1507f0d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract CIKs as a list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcik_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cik_key\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define target directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "hgVm0MbtmBHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    \"\"\"Remove HTML tags, attributes, and extra spaces from a document.\"\"\"\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    text = soup.get_text(separator=\" \")  # Extract text while preserving spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Path to the folder containing HTML files\n",
        "folder_path = \"/content/data/\"\n",
        "\n",
        "# Get a list of up to 40 HTML files in the folder\n",
        "html_files = glob.glob(os.path.join(folder_path, \"*.htm\"))[:40]  # Adjust if needed\n",
        "\n",
        "# Process each file\n",
        "cleaned_documents = {}\n",
        "\n",
        "for file_path in html_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    cleaned_text = clean_html(html_content)\n",
        "\n",
        "    # Store the cleaned text in a dictionary\n",
        "    cleaned_documents[file_path] = cleaned_text\n",
        "\n",
        "    # Optionally, save cleaned text to new files\n",
        "    output_file = file_path.replace(\".htm\", \"_cleaned.txt\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        out_file.write(cleaned_text)\n",
        "\n",
        "    print(f\"Processed: {file_path}\")\n",
        "\n",
        "# Print a preview of the first cleaned document\n",
        "print(\"\\nSample cleaned text:\\n\", list(cleaned_documents.values())[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdgK47Piloqt",
        "outputId": "d2d91ef6-c2f5-48b6-e1db-fd4d15334127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: /content/data/000149315222002707-form10-k.htm\n",
            "Processed: /content/data/000106299322001764-form10ka.htm\n",
            "Processed: /content/data/000000412722000002-swks-20211001.htm\n",
            "Processed: /content/data/000001292722000010-ba-20211231.htm\n",
            "Processed: /content/data/000155837022000635-gfe-20211031x10k.htm\n",
            "Processed: /content/data/000121390022004518-f10kt2021_homebistro.htm\n",
            "Processed: /content/data/000160706222000053-ibal063021formnt10k.htm\n",
            "Processed: /content/data/000162828022001439-lwenergy-20210930.htm\n",
            "Processed: /content/data/000143774922001938-bset20211127_10k.htm\n",
            "Processed: /content/data/000110465922009075-arteu-20211231x10k.htm\n",
            "Processed: /content/data/000173112222000120-e3461_nt10-k.htm\n",
            "Processed: /content/data/000157570522000076-puge_12b25.htm\n",
            "Processed: /content/data/000103544322000040-are-20211231.htm\n",
            "Processed: /content/data/000121390022004524-f10k2021_european.htm\n",
            "Processed: /content/data/000149315222002705-formnt10-k.htm\n",
            "Processed: /content/data/000110465922009084-twnd-20211231x10ka.htm\n",
            "Processed: /content/data/000139390522000029-dusyf_nt10k.htm\n",
            "Processed: /content/data/000108458022000006-jef-20211130.htm\n",
            "Processed: /content/data/000156459022003003-snx-10k_20211130.htm\n",
            "Processed: /content/data/000009622322000006-jef-20211130.htm\n",
            "Processed: /content/data/000119312522023409-d221966d10ka.htm\n",
            "Processed: /content/data/000164033422000218-xpl_nt10k.htm\n",
            "Processed: /content/data/000149315222002559-formnt10-k.htm\n",
            "Processed: /content/data/000149315222002765-form10-k.htm\n",
            "Processed: /content/data/000119312522023432-d445631d10ka.htm\n",
            "Processed: /content/data/000121390022004242-f10k2021a1_phenixfincorp.htm\n",
            "Processed: /content/data/000141057822000063-apdn-20210930x10ka.htm\n",
            "Processed: /content/data/000101738622000039-pedro_2021oct31-nt10k.htm\n",
            "Processed: /content/data/000182912622002166-organicell_nt10k.htm\n",
            "Processed: /content/data/000169298122000002-form12b25.htm\n",
            "Processed: /content/data/000126022122000004-tdg-20210930.htm\n",
            "Processed: /content/data/000121390022004241-f10k2021_dongfangcity.htm\n",
            "Processed: /content/data/000168316822000568-yuengling_i10k-103121.htm\n",
            "Processed: /content/data/000147793222000506-guskin_10k.htm\n",
            "Processed: /content/data/000162828022001450-len-20211130.htm\n",
            "Processed: /content/data/000009355622000004-swk-20210102.htm\n",
            "Processed: /content/data/000117494722000082-form10k-27201_frevs.htm\n",
            "Processed: /content/data/000156459022003007-nrix-10k_20211130.htm\n",
            "\n",
            "Sample cleaned text:\n",
            " 0001869974 false FY 0 0001869974 2021-06-17 2021-12-31 0001869974 AEHAU:UnitsEachConsistingOfOneShareOfClassCommonStockAndOneHalfOfOneRedeemableWarrantMember 2021-06-17 2021-12-31 0001869974 AEHAU:ClassCommonStockParValue0.0001PerShareMember 2021-06-17 2021-12-31 0001869974 AEHAU:WarrantsEachExercisableForOneShareOfClassCommonStockFor11.50PerShareMember 2021-06-17 2021-12-31 0001869974 2021-06-30 0001869974 us-gaap:CommonClassAMember 2022-01-28 0001869974 us-gaap:CommonClassBMember 2022-01-28 00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary nltk resources\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def load_dictionary(dictionary_path):\n",
        "    \"\"\"Load the 2of12inf dictionary into a set.\"\"\"\n",
        "    with open(dictionary_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        valid_words = set(word.strip().lower() for word in file)\n",
        "    return valid_words\n",
        "\n",
        "def tokenize_and_filter(text, dictionary):\n",
        "    \"\"\"Tokenize text and filter words based on the 2of12inf dictionary.\"\"\"\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.lower() in dictionary]  # Keep only dictionary words\n",
        "    return filtered_tokens\n",
        "\n",
        "# Load dictionary\n",
        "dictionary_path = \"2of12inf.txt\"  # Path to the 2of12inf dictionary file\n",
        "valid_words = load_dictionary(dictionary_path)\n",
        "\n",
        "# Example usage after cleaning HTML\n",
        "tokenized_text = tokenize_and_filter(cleaned_text, valid_words)\n",
        "print(tokenized_text[:50])  # Print first 50 tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SJd0tV_mUcj",
        "outputId": "3e586cc0-24cf-4338-d6c2-8c66aab2244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['true', 'no', 'true', 'shares', 'united', 'states', 'securities', 'and', 'exchange', 'commission', 'form', 'amendment', 'no', 'annual', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'securities', 'exchange', 'act', 'of', 'for', 'the', 'fiscal', 'year', 'ended', 'or', 'transition', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'securities', 'exchange', 'act', 'of', 'for', 'the', 'transition', 'period', 'from', 'commission', 'file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Ensure required nltk resources are available\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    \"\"\"Remove HTML tags, attributes, and extra spaces from a document.\"\"\"\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    text = soup.get_text(separator=\" \")  # Extract text while preserving spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def tokenize_and_filter(text, valid_words):\n",
        "    \"\"\"Tokenize the text and keep only words present in the valid_words dictionary.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    filtered_tokens = [word for word in tokens if word in valid_words]  # Filter words\n",
        "    return filtered_tokens\n",
        "\n",
        "# Load the 20of12inf dictionary\n",
        "dict_path = \"/content/2of12inf.txt\"\n",
        "with open(dict_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    valid_words = set(word.strip().lower() for word in file.readlines())  # Load words into a set\n",
        "\n",
        "# Path to the folder containing HTML files\n",
        "folder_path = \"/content/data/\"\n",
        "\n",
        "# Get a list of up to 40 HTML files in the folder\n",
        "html_files = glob.glob(os.path.join(folder_path, \"*.htm\"))[:40]\n",
        "\n",
        "# Process each file\n",
        "filtered_documents = {}\n",
        "\n",
        "for file_path in html_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    cleaned_text = clean_html(html_content)  # Step 1: Clean HTML\n",
        "    tokenized_text = tokenize_and_filter(cleaned_text, valid_words)  # Step 2: Tokenize & Filter\n",
        "\n",
        "    # Store the filtered tokens\n",
        "    filtered_documents[file_path] = tokenized_text\n",
        "\n",
        "    # Save filtered tokens to new files\n",
        "    output_file = file_path.replace(\".htm\", \"_filtered.txt\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        out_file.write(\" \".join(tokenized_text))  # Save as space-separated words\n",
        "\n",
        "    print(f\"Processed: {file_path}\")\n",
        "\n",
        "# Print a preview of the first filtered document\n",
        "print(\"\\nSample filtered tokens:\\n\", list(filtered_documents.values())[0][:50])  # Show first 50 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWavyEvYqI2t",
        "outputId": "9a83e363-a032-41d9-d8d9-108ac4f05414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: /content/data/000149315222002707-form10-k.htm\n",
            "Processed: /content/data/000106299322001764-form10ka.htm\n",
            "Processed: /content/data/000000412722000002-swks-20211001.htm\n",
            "Processed: /content/data/000001292722000010-ba-20211231.htm\n",
            "Processed: /content/data/000155837022000635-gfe-20211031x10k.htm\n",
            "Processed: /content/data/000121390022004518-f10kt2021_homebistro.htm\n",
            "Processed: /content/data/000160706222000053-ibal063021formnt10k.htm\n",
            "Processed: /content/data/000162828022001439-lwenergy-20210930.htm\n",
            "Processed: /content/data/000143774922001938-bset20211127_10k.htm\n",
            "Processed: /content/data/000110465922009075-arteu-20211231x10k.htm\n",
            "Processed: /content/data/000173112222000120-e3461_nt10-k.htm\n",
            "Processed: /content/data/000157570522000076-puge_12b25.htm\n",
            "Processed: /content/data/000103544322000040-are-20211231.htm\n",
            "Processed: /content/data/000121390022004524-f10k2021_european.htm\n",
            "Processed: /content/data/000149315222002705-formnt10-k.htm\n",
            "Processed: /content/data/000110465922009084-twnd-20211231x10ka.htm\n",
            "Processed: /content/data/000139390522000029-dusyf_nt10k.htm\n",
            "Processed: /content/data/000108458022000006-jef-20211130.htm\n",
            "Processed: /content/data/000156459022003003-snx-10k_20211130.htm\n",
            "Processed: /content/data/000009622322000006-jef-20211130.htm\n",
            "Processed: /content/data/000119312522023409-d221966d10ka.htm\n",
            "Processed: /content/data/000164033422000218-xpl_nt10k.htm\n",
            "Processed: /content/data/000149315222002559-formnt10-k.htm\n",
            "Processed: /content/data/000149315222002765-form10-k.htm\n",
            "Processed: /content/data/000119312522023432-d445631d10ka.htm\n",
            "Processed: /content/data/000121390022004242-f10k2021a1_phenixfincorp.htm\n",
            "Processed: /content/data/000141057822000063-apdn-20210930x10ka.htm\n",
            "Processed: /content/data/000101738622000039-pedro_2021oct31-nt10k.htm\n",
            "Processed: /content/data/000182912622002166-organicell_nt10k.htm\n",
            "Processed: /content/data/000169298122000002-form12b25.htm\n",
            "Processed: /content/data/000126022122000004-tdg-20210930.htm\n",
            "Processed: /content/data/000121390022004241-f10k2021_dongfangcity.htm\n",
            "Processed: /content/data/000168316822000568-yuengling_i10k-103121.htm\n",
            "Processed: /content/data/000147793222000506-guskin_10k.htm\n",
            "Processed: /content/data/000162828022001450-len-20211130.htm\n",
            "Processed: /content/data/000009355622000004-swk-20210102.htm\n",
            "Processed: /content/data/000117494722000082-form10k-27201_frevs.htm\n",
            "Processed: /content/data/000156459022003007-nrix-10k_20211130.htm\n",
            "\n",
            "Sample filtered tokens:\n",
            " ['false', 'shares', 'shares', 'pure', 'united', 'states', 'securities', 'and', 'exchange', 'commission', 'form', 'mark', 'one', 'annual', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'securities', 'exchange', 'act', 'of', 'for', 'the', 'fiscal', 'year', 'ended', 'transition', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'securities', 'exchange', 'act', 'of', 'for', 'the', 'transition', 'period', 'from', 'to', 'commission', 'file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def apply_stemming(words):\n",
        "    \"\"\"Apply Porter Stemming to a list of words.\"\"\"\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Path to filtered text files\n",
        "filtered_files = glob.glob(\"/content/data/*_filtered.txt\")\n",
        "\n",
        "for file_path in filtered_files:\n",
        "    # Read the filtered words\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        words = file.read().split()  # Read as a list of words\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed_words = apply_stemming(words)\n",
        "\n",
        "    # Save stemmed words to a new file\n",
        "    output_file = file_path.replace(\"_filtered.txt\", \"_stemmed.txt\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        out_file.write(\" \".join(stemmed_words))  # Save as space-separated words\n",
        "\n",
        "    print(f\"Stemmed file saved: {output_file}\")\n",
        "\n",
        "# Print a preview of the first stemmed document\n",
        "sample_file = filtered_files[0].replace(\"_filtered.txt\", \"_stemmed.txt\")\n",
        "with open(sample_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    print(\"\\nSample Stemmed Words:\\n\", file.read().split()[:50])  # Show first 50 words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMQgUSBzrByJ",
        "outputId": "b53b68a9-dd67-47c1-fd3d-dbdcd59502dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed file saved: /content/data/000121390022004524-f10k2021_european_stemmed.txt\n",
            "Stemmed file saved: /content/data/000000412722000002-swks-20211001_stemmed.txt\n",
            "Stemmed file saved: /content/data/000139390522000029-dusyf_nt10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000160706222000053-ibal063021formnt10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000101738622000039-pedro_2021oct31-nt10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000169298122000002-form12b25_stemmed.txt\n",
            "Stemmed file saved: /content/data/000149315222002765-form10-k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000110465922009084-twnd-20211231x10ka_stemmed.txt\n",
            "Stemmed file saved: /content/data/000147793222000506-guskin_10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000168316822000568-yuengling_i10k-103121_stemmed.txt\n",
            "Stemmed file saved: /content/data/000110465922009075-arteu-20211231x10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000121390022004518-f10kt2021_homebistro_stemmed.txt\n",
            "Stemmed file saved: /content/data/000156459022003007-nrix-10k_20211130_stemmed.txt\n",
            "Stemmed file saved: /content/data/000117494722000082-form10k-27201_frevs_stemmed.txt\n",
            "Stemmed file saved: /content/data/000103544322000040-are-20211231_stemmed.txt\n",
            "Stemmed file saved: /content/data/000141057822000063-apdn-20210930x10ka_stemmed.txt\n",
            "Stemmed file saved: /content/data/000162828022001439-lwenergy-20210930_stemmed.txt\n",
            "Stemmed file saved: /content/data/000121390022004241-f10k2021_dongfangcity_stemmed.txt\n",
            "Stemmed file saved: /content/data/000121390022004242-f10k2021a1_phenixfincorp_stemmed.txt\n",
            "Stemmed file saved: /content/data/000126022122000004-tdg-20210930_stemmed.txt\n",
            "Stemmed file saved: /content/data/000108458022000006-jef-20211130_stemmed.txt\n",
            "Stemmed file saved: /content/data/000149315222002707-form10-k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000009355622000004-swk-20210102_stemmed.txt\n",
            "Stemmed file saved: /content/data/000155837022000635-gfe-20211031x10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000164033422000218-xpl_nt10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000119312522023409-d221966d10ka_stemmed.txt\n",
            "Stemmed file saved: /content/data/000119312522023432-d445631d10ka_stemmed.txt\n",
            "Stemmed file saved: /content/data/000143774922001938-bset20211127_10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000009622322000006-jef-20211130_stemmed.txt\n",
            "Stemmed file saved: /content/data/000106299322001764-form10ka_stemmed.txt\n",
            "Stemmed file saved: /content/data/000182912622002166-organicell_nt10k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000162828022001450-len-20211130_stemmed.txt\n",
            "Stemmed file saved: /content/data/000157570522000076-puge_12b25_stemmed.txt\n",
            "Stemmed file saved: /content/data/000001292722000010-ba-20211231_stemmed.txt\n",
            "Stemmed file saved: /content/data/000156459022003003-snx-10k_20211130_stemmed.txt\n",
            "Stemmed file saved: /content/data/000149315222002705-formnt10-k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000149315222002559-formnt10-k_stemmed.txt\n",
            "Stemmed file saved: /content/data/000173112222000120-e3461_nt10-k_stemmed.txt\n",
            "\n",
            "Sample Stemmed Words:\n",
            " ['unit', 'state', 'secur', 'and', 'exchang', 'commiss', 'form', 'mark', 'one', 'annual', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'secur', 'exchang', 'act', 'of', 'for', 'the', 'fiscal', 'year', 'end', 'or', 'transit', 'report', 'pursuant', 'to', 'section', 'or', 'of', 'the', 'secur', 'exchang', 'act', 'of', 'for', 'the', 'transit', 'period', 'from', 'to', 'commiss', 'file', 'number', 'sustain', 'growth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "# Load ESG Dictionary (the stemmed words)\n",
        "ws = pd.read_csv(\"stemmed_words.csv\")\n",
        "esg_words = set(ws['Stemmed_Words'])  # ESG words as a set\n",
        "\n",
        "# Get list of stemmed text files\n",
        "stemmed_files = glob.glob(\"/content/data/*_stemmed.txt\")\n",
        "\n",
        "# Initialize an empty DataFrame to hold the matrix\n",
        "matrix = pd.DataFrame(columns=sorted(esg_words))  # Columns are ESG words\n",
        "\n",
        "# Process each stemmed document\n",
        "for file_path in stemmed_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        # Read the document and tokenize into words\n",
        "        document_words = file.read().split()\n",
        "\n",
        "        # Count the frequency of each word in the document\n",
        "        word_counts = Counter(document_words)\n",
        "\n",
        "        # Calculate the total word count (including ESG and non-ESG words)\n",
        "        total_word_count = len(document_words)\n",
        "\n",
        "    # Create a row for the current document with frequency of each ESG word\n",
        "    row = {word: word_counts.get(word, 0) for word in esg_words}\n",
        "\n",
        "    # Add the total word count to the row\n",
        "    row['Total_Word_Count'] = total_word_count\n",
        "\n",
        "    # Append the row to the matrix (DataFrame)\n",
        "    matrix = pd.concat([matrix, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "# Save matrix to a CSV file\n",
        "matrix.to_csv(\"/content/data/word_matrix.csv\", index=False)\n",
        "print(\"Word matrix saved: /content/data/word_matrix.csv\")\n",
        "\n",
        "# Display the first 5 rows to confirm\n",
        "print(matrix.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWJMuGi52CeB",
        "outputId": "cb5ff653-6acb-421c-f558-475563cd27d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word matrix saved: /content/data/word_matrix.csv\n",
            "  agricultur air alcohol align announc appreci approv asc assess atmospher  \\\n",
            "0          0   0       0    18       4       2    105   0     19         0   \n",
            "1          0   0       0     0       1       2     78   0     13         0   \n",
            "2          0   0       0     0       0       0      0   0      0         0   \n",
            "3          0   0       0     0       0       0      2   0      3         0   \n",
            "4          0   0       0     0       2       4    118   0     11         0   \n",
            "\n",
            "   ... wetland whistleblow wilder wildlif wind woman women workplac zone  \\\n",
            "0  ...       0           0      0       0    6     0     1        0    0   \n",
            "1  ...       0           0      0       0    5     0     0        0    0   \n",
            "2  ...       0           0      0       0    0     0     0        0    0   \n",
            "3  ...       0           0      0       0    0     0     0        0    0   \n",
            "4  ...       0           0      0       0   11     0     2        0    0   \n",
            "\n",
            "  Total_Word_Count  \n",
            "0          68386.0  \n",
            "1          49863.0  \n",
            "2            497.0  \n",
            "3          10752.0  \n",
            "4          70085.0  \n",
            "\n",
            "[5 rows x 303 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load word matrix CSV (each row represents a report and each column is an ESG word frequency)\n",
        "word_matrix = pd.read_csv('/content/data/word_matrix.csv')\n",
        "\n",
        "# Load abnormal returns (for example, if these are stored in a list)\n",
        "# Assuming you have 38 abnormal return values corresponding to the 38 reports\n",
        "abnormal_returns = np.array([0.02, 0.03, -0.01, 0.05, 0.01, -0.02, 0.03, -0.04, 0.02, 0.01,\n",
        "                             0.03, -0.01, 0.02, 0.04, -0.03, 0.01, 0.02, 0.03, -0.01, 0.02,\n",
        "                             0.01, 0.03, 0.05, -0.02, 0.03, 0.04, -0.01, 0.02, 0.01, 0.03,\n",
        "                             0.02, -0.01, 0.04, 0.01, 0.02, 0.03, 0.01, 0.02])  # Example values\n",
        "\n",
        "# Ensure the number of abnormal returns matches the number of reports\n",
        "len(abnormal_returns) == word_matrix.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17HwwikJ6Avo",
        "outputId": "51898b22-0f67-40d0-be91-184056968e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Add intercept (constant term) to the feature matrix\n",
        "# X = sm.add_constant(word_matrix)\n",
        "\n",
        "# # Check the shape of X to ensure it matches the data\n",
        "# print(X.shape)  # It should have 39 columns (1 intercept + number of ESG words)\n",
        "# print(X.head)\n",
        "# # Fit OLS regression model\n",
        "# ols_model = sm.OLS(abnormal_returns, X)\n",
        "# ols_results = ols_model.fit()\n",
        "\n",
        "# # Output the optimized weights (coefficients)\n",
        "# optimized_weights = ols_results.params\n",
        "# print(\"Optimized Weights:\", optimized_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX0obQ2H6ilm",
        "outputId": "7bb9ac17-d888-4b42-9fb8-d8d4fdb7eedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38, 303)\n",
            "<bound method NDFrame.head of     const  agricultur  air  alcohol  align  announc  appreci  approv  asc  \\\n",
            "0     1.0           0    0        0     18        4        2     105    0   \n",
            "1     1.0           0    0        0      0        1        2      78    0   \n",
            "2     1.0           0    0        0      0        0        0       0    0   \n",
            "3     1.0           0    0        0      0        0        0       2    0   \n",
            "4     1.0           0    0        0      0        2        4     118    0   \n",
            "5     1.0           3    9        0      3       13        1      37    0   \n",
            "6     1.0           0    0        0      5        0        0      41    0   \n",
            "7     1.0           0    0        0      0        0        0       0    0   \n",
            "8     1.0           0    0        0      3        7        1     111    0   \n",
            "9     1.0           0    0        0      4        1        2       2    0   \n",
            "10    1.0           1   24        0      3       14        1      27    0   \n",
            "11    1.0           0    1        0      0        0        0       0    0   \n",
            "12    1.0           0    0        0      1        0        2      12    0   \n",
            "13    1.0           0    0        0      0        0        0      17    0   \n",
            "14    1.0           0    0        0      0        0        0       0    0   \n",
            "15    1.0          11    0        0      0        0        0       3    0   \n",
            "16    1.0          15    4        2      0        2        0      16    0   \n",
            "17    1.0           1    0        0      0        1        1       8    0   \n",
            "18    1.0           0    1        0      4        6        3      17    0   \n",
            "19    1.0           0    0        0      0        0        0       0    0   \n",
            "20    1.0           0    0        0      7        0        0      15    0   \n",
            "21    1.0           0    0        0      0        0        0       1    0   \n",
            "22    1.0           0    0        0      0        0        0       2    0   \n",
            "23    1.0           0    0        0      0        0        0       0    0   \n",
            "24    1.0           1    0        0      6        2        0      14    0   \n",
            "25    1.0           0    3        0      8        0        0       7    0   \n",
            "26    1.0           0    0        0      0        0        0       0    0   \n",
            "27    1.0           2    0        0      0        3        0      16    0   \n",
            "28    1.0           2    0        0      4        5        7      17    0   \n",
            "29    1.0           0    0        0      0        0        0       0    0   \n",
            "30    1.0           0    0        0      2       13        3     447    0   \n",
            "31    1.0           0    0        0      0        0        0       0    0   \n",
            "32    1.0           0    0        0      3        3        3      18    0   \n",
            "33    1.0           0    1        0      0        5        9      16    0   \n",
            "34    1.0           1    0        0      0        2        0     120    0   \n",
            "35    1.0           0    0        0      5       12        3      15    0   \n",
            "36    1.0           0    0        0      0        0        0       0    0   \n",
            "37    1.0           0    0        0      1        0        0       7    0   \n",
            "\n",
            "    assess  ...  welfar  wetland  whistleblow  wilder  wildlif  wind  woman  \\\n",
            "0       19  ...       0        0            0       0        0     6      0   \n",
            "1       13  ...       0        0            0       0        0     5      0   \n",
            "2        0  ...       0        0            0       0        0     0      0   \n",
            "3        3  ...       0        0            0       0        0     0      0   \n",
            "4       11  ...       0        0            0       0        0    11      0   \n",
            "5       69  ...       0        0            0       0        0     9      0   \n",
            "6       17  ...       0        0            0       0        0     0      0   \n",
            "7        0  ...       0        0            0       0        0     0      0   \n",
            "8       16  ...       0        0            0       0        0     7      0   \n",
            "9       30  ...       0        0            0       0        0     0      0   \n",
            "10      50  ...       0        0            0       0        0     0      0   \n",
            "11       5  ...       0        0            0       0        0     0      0   \n",
            "12       5  ...       0        0            0       0        0     0      0   \n",
            "13       6  ...       0        0            0       0        0     1      0   \n",
            "14       0  ...       0        0            0       0        0     0      0   \n",
            "15       1  ...       0        0            0       0        0     0      0   \n",
            "16      10  ...       0        0            0       0        0     0      0   \n",
            "17      14  ...       0        0            0       0        0     0      0   \n",
            "18      62  ...       0        0            0       0        0     0      0   \n",
            "19       0  ...       0        0            0       0        0     0      0   \n",
            "20       2  ...       1        0            0       0        0     0      0   \n",
            "21       0  ...       0        0            0       0        0     0      0   \n",
            "22       8  ...       0        0            0       0        0     0      0   \n",
            "23       0  ...       0        0            0       0        0     0      0   \n",
            "24      12  ...       0        0            0       0        0     0      0   \n",
            "25       5  ...       0        0            0       0        0     0      0   \n",
            "26       0  ...       0        0            0       0        0     0      0   \n",
            "27      28  ...       0        0            0       0        0     0      0   \n",
            "28      55  ...       0        0            0       0        0     4      0   \n",
            "29       0  ...       0        0            0       0        0     0      0   \n",
            "30      52  ...       1        0            0       0        0     0      0   \n",
            "31       0  ...       0        0            0       0        0     0      0   \n",
            "32      49  ...       0        0            0       0        0     3      0   \n",
            "33      29  ...       0        1            0       0        0     0      0   \n",
            "34      20  ...       0        0            0       0        0     6      0   \n",
            "35      29  ...       0        0            0       0        0     0      0   \n",
            "36       0  ...       0        0            0       0        0     0      0   \n",
            "37       1  ...       1        0            0       0        0     0      0   \n",
            "\n",
            "    women  workplac  zone  \n",
            "0       1         0     0  \n",
            "1       0         0     0  \n",
            "2       0         0     0  \n",
            "3       0         0     0  \n",
            "4       2         0     0  \n",
            "5       1         1     4  \n",
            "6       0         0     8  \n",
            "7       0         0     0  \n",
            "8       0         0     0  \n",
            "9       0         2     0  \n",
            "10      1         4     0  \n",
            "11      0         0     1  \n",
            "12      0         0     0  \n",
            "13      0         0     0  \n",
            "14      0         0     0  \n",
            "15      0         0     1  \n",
            "16      0         0     0  \n",
            "17      0         0     0  \n",
            "18      5         5     0  \n",
            "19      0         0     0  \n",
            "20      0         0     0  \n",
            "21      0         0     0  \n",
            "22      0         0     3  \n",
            "23      0         0     0  \n",
            "24      0         0     0  \n",
            "25      0         0     0  \n",
            "26      0         0     0  \n",
            "27      1         0     0  \n",
            "28      1         1     0  \n",
            "29      0         0     0  \n",
            "30      0         2     1  \n",
            "31      0         0     0  \n",
            "32      1         1     0  \n",
            "33      0         0     3  \n",
            "34      0         0     0  \n",
            "35      1         3     0  \n",
            "36      0         0     0  \n",
            "37      3         0     0  \n",
            "\n",
            "[38 rows x 303 columns]>\n",
            "Optimized Weights: const        -0.009501\n",
            "agricultur    0.001431\n",
            "air          -0.002915\n",
            "alcohol      -0.000271\n",
            "align         0.008134\n",
            "                ...   \n",
            "wind          0.001711\n",
            "woman         0.000000\n",
            "women         0.001730\n",
            "workplac      0.000266\n",
            "zone         -0.002271\n",
            "Length: 303, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Load the word frequency matrix (word_matrix.csv)\n",
        "word_matrix = pd.read_csv('/content/data/word_matrix.csv')  # Assuming first column is report names\n",
        "\n",
        "# Step 2: Set initial weights to 1 for each ESG word\n",
        "word_weights = {word: 1 for word in word_matrix.columns if word != 'Total_Word_Count'}  # Exclude 'Total_Word_Count' column\n",
        "\n",
        "# Step 3: Calculate the ESG Sentiment Score for each report\n",
        "def calculate_esg_sentiment(word_matrix, word_weights):\n",
        "    sentiment_scores = []\n",
        "    for report, row in word_matrix.iterrows():\n",
        "        # Get the frequency of each ESG word in the report (F_i,j)\n",
        "        frequencies = row.drop('Total_Word_Count').values  # Exclude 'Total_Word_Count' column\n",
        "\n",
        "        # Get the total number of words in the report (Total_Word_Count column)\n",
        "        total_words = row['Total_Word_Count']  # Using the Total_Word_Count column\n",
        "\n",
        "        # Calculate the weighted sum for the ESG sentiment score\n",
        "        weighted_sum = 0\n",
        "        for word, freq in zip(word_weights.keys(), frequencies):\n",
        "            weighted_sum += word_weights[word] * freq\n",
        "\n",
        "        # Calculate ESG sentiment score\n",
        "        esg_sentiment = weighted_sum / total_words if total_words > 0 else 0\n",
        "        sentiment_scores.append(esg_sentiment)\n",
        "\n",
        "    # Return a DataFrame of sentiment scores for each report\n",
        "    return pd.Series(sentiment_scores, index=word_matrix.index, name='ESG Sentiment Score')\n",
        "\n",
        "# Step 4: Calculate ESG sentiment scores with initial weights\n",
        "esg_scores_initial = calculate_esg_sentiment(word_matrix, word_weights)\n",
        "print(\"Initial ESG Sentiment Scores:\")\n",
        "print(esg_scores_initial)\n",
        "\n",
        "# Step 5: Now, perform OLS regression to refine word weights based on market reactions\n",
        "# Assuming you have the abnormal returns (Δr_i) data for each report\n",
        "# You can load the abnormal returns data as follows:\n",
        "#abnormal_returns = pd.read_csv('abnormal_returns.csv', index_col=0)  # Assuming same index as word_matrix\n",
        "\n",
        "# Ensure that the number of reports in abnormal_returns matches word_matrix\n",
        "assert abnormal_returns.shape[0] == word_matrix.shape[0], \"Mismatch in number of reports\"\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (y)\n",
        "# X will be the word frequencies for each report, and y will be the abnormal returns\n",
        "X = word_matrix.drop(columns=['Total_Word_Count']).values  # Exclude 'Total_Word_Count' column\n",
        "y = abnormal_returns  # Directly flatten without using `.values`\n",
        "\n",
        "# Add a constant to the X matrix for the intercept term in OLS\n",
        "X_with_intercept = sm.add_constant(X)\n",
        "\n",
        "# Step 6: Fit the OLS model to find the best word weights\n",
        "ols_model = sm.OLS(y, X_with_intercept)\n",
        "ols_results = ols_model.fit()\n",
        "\n",
        "# Get the estimated coefficients (including the intercept)\n",
        "estimated_weights = ols_results.params[1:]  # Exclude the intercept\n",
        "word_weights_ols = {word: weight for word, weight in zip(word_matrix.columns[:-1], estimated_weights)}  # Exclude 'Total_Word_Count'\n",
        "\n",
        "# Step 7: Calculate ESG sentiment scores again using the refined weights from OLS\n",
        "esg_scores_refined = calculate_esg_sentiment(word_matrix, word_weights_ols)\n",
        "print(\"Refined ESG Sentiment Scores (after OLS):\")\n",
        "print(esg_scores_refined)\n",
        "\n",
        "# Optionally: Save the results to CSV\n",
        "esg_scores_initial.to_csv('esg_sentiment_scores_initial.csv', header=True)\n",
        "esg_scores_refined.to_csv('esg_sentiment_scores_refined.csv', header=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeBuyQYV2ZI-",
        "outputId": "44e8f036-148d-4098-f092-f9dc89f36390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial ESG Sentiment Scores:\n",
            "0     0.027111\n",
            "1     0.027515\n",
            "2     0.018109\n",
            "3     0.031529\n",
            "4     0.026525\n",
            "5     0.031841\n",
            "6     0.026810\n",
            "7     0.022263\n",
            "8     0.026763\n",
            "9     0.031401\n",
            "10    0.035621\n",
            "11    0.030518\n",
            "12    0.074166\n",
            "13    0.034277\n",
            "14    0.021484\n",
            "15    0.046380\n",
            "16    0.022871\n",
            "17    0.038816\n",
            "18    0.030326\n",
            "19    0.018587\n",
            "20    0.065803\n",
            "21    0.018779\n",
            "22    0.040209\n",
            "23    0.016949\n",
            "24    0.029164\n",
            "25    0.047006\n",
            "26    0.022018\n",
            "27    0.032027\n",
            "28    0.031594\n",
            "29    0.020142\n",
            "30    0.044592\n",
            "31    0.017886\n",
            "32    0.033342\n",
            "33    0.031874\n",
            "34    0.027736\n",
            "35    0.032728\n",
            "36    0.025292\n",
            "37    0.053405\n",
            "Name: ESG Sentiment Score, dtype: float64\n",
            "Refined ESG Sentiment Scores (after OLS):\n",
            "0     4.313949e-07\n",
            "1     7.921981e-07\n",
            "2    -1.003273e-06\n",
            "3     5.533982e-06\n",
            "4     2.782532e-07\n",
            "5    -1.075580e-07\n",
            "6     7.278542e-07\n",
            "7    -5.658372e-05\n",
            "8     4.566564e-07\n",
            "9     6.166832e-07\n",
            "10    7.197379e-07\n",
            "11   -3.842684e-08\n",
            "12    3.064441e-06\n",
            "13    2.238463e-06\n",
            "14   -4.003638e-05\n",
            "15    3.076412e-06\n",
            "16    9.345045e-07\n",
            "17    1.812405e-06\n",
            "18   -7.238643e-09\n",
            "19    5.483527e-05\n",
            "20    1.185998e-06\n",
            "21    6.181749e-05\n",
            "22    3.341460e-06\n",
            "23   -1.617662e-05\n",
            "24    6.939927e-07\n",
            "25    2.188970e-06\n",
            "26   -9.149114e-07\n",
            "27    1.118154e-06\n",
            "28    2.298091e-07\n",
            "29    4.680257e-05\n",
            "30    3.246619e-07\n",
            "31   -8.107751e-07\n",
            "32    7.719753e-07\n",
            "33    4.163313e-07\n",
            "34    4.538324e-07\n",
            "35    7.727337e-07\n",
            "36    3.794041e-05\n",
            "37    3.282307e-06\n",
            "Name: ESG Sentiment Score, dtype: float64\n"
          ]
        }
      ]
    }
  ]
}